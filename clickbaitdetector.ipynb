{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader \nfrom transformers import DistilBertTokenizer, DistilBertModel","metadata":{"execution":{"iopub.status.busy":"2024-01-27T09:12:17.442300Z","iopub.execute_input":"2024-01-27T09:12:17.443012Z","iopub.status.idle":"2024-01-27T09:12:17.448047Z","shell.execute_reply.started":"2024-01-27T09:12:17.442979Z","shell.execute_reply":"2024-01-27T09:12:17.447093Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!wget https://ml-coding-test.s3.eu-west-1.amazonaws.com/webis_train.csv","metadata":{"execution":{"iopub.status.busy":"2024-01-27T09:12:22.544015Z","iopub.execute_input":"2024-01-27T09:12:22.544365Z","iopub.status.idle":"2024-01-27T09:12:31.655864Z","shell.execute_reply.started":"2024-01-27T09:12:22.544339Z","shell.execute_reply":"2024-01-27T09:12:31.654561Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"--2024-01-27 09:12:23--  https://ml-coding-test.s3.eu-west-1.amazonaws.com/webis_train.csv\nResolving ml-coding-test.s3.eu-west-1.amazonaws.com (ml-coding-test.s3.eu-west-1.amazonaws.com)... 52.218.90.248, 3.5.70.193, 52.218.116.234, ...\nConnecting to ml-coding-test.s3.eu-west-1.amazonaws.com (ml-coding-test.s3.eu-west-1.amazonaws.com)|52.218.90.248|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 93472103 (89M) [text/csv]\nSaving to: 'webis_train.csv.1'\n\nwebis_train.csv.1   100%[===================>]  89.14M  16.4MB/s    in 7.1s    \n\n2024-01-27 09:12:31 (12.6 MB/s) - 'webis_train.csv.1' saved [93472103/93472103]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/working/webis_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T08:56:28.172280Z","iopub.execute_input":"2024-01-27T08:56:28.172681Z","iopub.status.idle":"2024-01-27T08:56:29.457340Z","shell.execute_reply.started":"2024-01-27T08:56:28.172649Z","shell.execute_reply":"2024-01-27T08:56:29.456282Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-01-27T09:12:39.184476Z","iopub.execute_input":"2024-01-27T09:12:39.184914Z","iopub.status.idle":"2024-01-27T09:12:39.218007Z","shell.execute_reply.started":"2024-01-27T09:12:39.184873Z","shell.execute_reply":"2024-01-27T09:12:39.217030Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0                               postMedia  \\\n0           4795                                      []   \n1           4979                                      []   \n2           1448  ['media/photo_818125834103857152.jpg']   \n3            381                                      []   \n4           7226                                      []   \n...          ...                                     ...   \n9756        1296  ['media/photo_822251094533292032.jpg']   \n9757         575  ['media/photo_842072978926010368.jpg']   \n9758         840                                      []   \n9759        3527                                      []   \n9760        3305                                      []   \n\n                                               postText                  id  \\\n0     Tensions are high in Jewish communities across...  833993791157723136   \n1     Turkey dismisses 4,400 public servants in late...  829279322729021440   \n2     21 \"Law And Order\" Tumblr posts that are fucki...  818185517921632256   \n3     How one woman is using Tinder to take advantag...  847327280061919232   \n4                     How the dinosaurs REALLY died out  820869672488890368   \n...                                                 ...                 ...   \n9756  This is what 100 years of women’s protest look...  822420412340387840   \n9757  One #WBC2017 fan’s attempt to catch a home run...  842074484026798080   \n9758  Hunt master investigated by police after alleg...  834432577184292864   \n9759  \"In 2016’s politics and pop culture, women cam...  815386319782342656   \n9760  The GOP’s dilemma: As Obamacare repeal nears, ...  821458375280119808   \n\n                                         targetCaptions  \\\n0     ['Jewish cemetery vandalized, JCCs threatened'...   \n1     ['Turkish President Erdogan reviews a guard of...   \n2                                                    []   \n3                              ['istock-476739206.jpg']   \n4     [\"Researchers say the 'big chill' had far more...   \n...                                                 ...   \n9756                                                 []   \n9757                                                 []   \n9758  ['Hunt master investigated after allegedly whi...   \n9759  ['Dolores (Evan Rachel Wood) embraces her iden...   \n9760  ['A woman holds an Obamacare sign in front of ...   \n\n                                       targetParagraphs  \\\n0     ['(CNN)Tensions remain high in Jewish communit...   \n1     ['Dismissals come hours after first phone call...   \n2     ['If SVU isn’t your favorite, are you even a r...   \n3     ['One user has received money from more than 2...   \n4     ['The apocalyptic time that many believe wiped...   \n...                                                 ...   \n9756  ['“Life’s a bitch. You’ve got to go out and ki...   \n9757  ['When\\xa0Yurendell de Caster of the Netherlan...   \n9758  ['A hunt master has been reported to police af...   \n9759  ['This year, rather than making best-of lists,...   \n9760  [\"We are closer than we have ever been to the ...   \n\n                                            targetTitle  \\\n0     Vandals damage 100 headstones at Jewish cemete...   \n1     Turkey dismisses 4,400 public servants in late...   \n2     21 \"Law And Order\" Tumblr Posts That Are Fucki...   \n3     How women are using their Tinder matches to sc...   \n4     Is this how the dinosaurs REALLY died out? Res...   \n...                                                 ...   \n9756  This Is What 100 Years Of Women’s Protest Look...   \n9757  Netherlands Home Run Hits World Baseball Class...   \n9758  Hunt master investigated by police after alleg...   \n9759  In 2016’s politics and pop culture, women came...   \n9760  The GOP’s dilemma: As Obamacare repeal nears, ...   \n\n                       postTimestamp  \\\n0     Tue Feb 21 10:56:29 +0000 2017   \n1     Wed Feb 08 10:42:52 +0000 2017   \n2     Sun Jan 08 20:00:03 +0000 2017   \n3     Thu Mar 30 05:59:00 +0000 2017   \n4     Mon Jan 16 05:45:55 +0000 2017   \n...                              ...   \n9756  Fri Jan 20 12:28:00 +0000 2017   \n9757  Wed Mar 15 18:06:16 +0000 2017   \n9758  Wed Feb 22 16:00:04 +0000 2017   \n9759  Sun Jan 01 02:37:02 +0000 2017   \n9760  Tue Jan 17 20:45:13 +0000 2017   \n\n                                         targetKeywords  \\\n0     us, Vandals damage 100 headstones at Jewish ce...   \n1     Turkey,Recep Tayyip Erdoğan,UK news,Donald Tru...   \n2                                                   NaN   \n3     Tinder, dating apps, Dating, scam, Online dati...   \n4     A,dark,frozen,Earth,Researchers,reconstruct,ap...   \n...                                                 ...   \n9756                                                NaN   \n9757                                 MLB, Breaking News   \n9758  News,Surrey,Surrey Police,Hunting,Standard,UK ...   \n9759  Hillary Clinton, Donald Trump, Game of Thrones...   \n9760  the fix, obamacare, affordable care act, obama...   \n\n                                      targetDescription  \\\n0     Tensions remain high in Jewish communities acr...   \n1     Dismissals come hours after first phone call b...   \n2                                                   NaN   \n3     How to create the perfect Tinder bio is one of...   \n4     Researchers say the 'big chill' had far more c...   \n...                                                 ...   \n9756                                                NaN   \n9757  When Yurendell de Caster of the Netherlands la...   \n9758  A hunt master has been reported to police afte...   \n9759  Hillary Clinton, Cersei Lannister, Dolores Abe...   \n9760  Trump promised “insurance for everybody.” And ...   \n\n                                         truthJudgments  truthMean  \\\n0                             [0.0, 1.0, 0.0, 0.0, 0.0]   0.200000   \n1             [0.0, 0.0, 0.33333333330000003, 0.0, 0.0]   0.066667   \n2     [0.6666666666000001, 0.33333333330000003, 1.0,...   0.600000   \n3     [0.33333333330000003, 0.0, 1.0, 1.0, 0.6666666...   0.600000   \n4             [0.33333333330000003, 1.0, 0.0, 1.0, 1.0]   0.666667   \n...                                                 ...        ...   \n9756  [1.0, 0.33333333330000003, 0.33333333330000003...   0.466667   \n9757  [0.33333333330000003, 0.6666666666000001, 0.33...   0.466667   \n9758  [0.33333333330000003, 0.0, 0.6666666666000001,...   0.400000   \n9759          [1.0, 0.0, 1.0, 1.0, 0.33333333330000003]   0.666667   \n9760          [0.33333333330000003, 1.0, 0.0, 0.0, 0.0]   0.266667   \n\n        truthClass  truthMedian  truthMode  \\\n0     no-clickbait     0.000000   0.000000   \n1     no-clickbait     0.000000   0.000000   \n2        clickbait     0.666667   1.000000   \n3        clickbait     0.666667   1.000000   \n4        clickbait     1.000000   1.000000   \n...            ...          ...        ...   \n9756  no-clickbait     0.333333   0.333333   \n9757  no-clickbait     0.333333   0.333333   \n9758  no-clickbait     0.333333   0.333333   \n9759     clickbait     1.000000   1.000000   \n9760  no-clickbait     0.000000   0.000000   \n\n                                         cleaned_string  \n0     ['(cnn)tensions remain high in jewish communit...  \n1     ['dismissals come hours after first phone call...  \n2     ['if svu isn’t your favorite, are you even a r...  \n3     ['one user has received money from more than 2...  \n4     ['the apocalyptic time that many believe wiped...  \n...                                                 ...  \n9756  ['“life’s a bitch. you’ve got to go out and ki...  \n9757  ['when yurendell de caster of the netherlands ...  \n9758  ['a hunt master has been reported to police af...  \n9759  ['this year, rather than making best-of lists,...  \n9760  [\"we are closer than we have ever been to the ...  \n\n[9761 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>postMedia</th>\n      <th>postText</th>\n      <th>id</th>\n      <th>targetCaptions</th>\n      <th>targetParagraphs</th>\n      <th>targetTitle</th>\n      <th>postTimestamp</th>\n      <th>targetKeywords</th>\n      <th>targetDescription</th>\n      <th>truthJudgments</th>\n      <th>truthMean</th>\n      <th>truthClass</th>\n      <th>truthMedian</th>\n      <th>truthMode</th>\n      <th>cleaned_string</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4795</td>\n      <td>[]</td>\n      <td>Tensions are high in Jewish communities across...</td>\n      <td>833993791157723136</td>\n      <td>['Jewish cemetery vandalized, JCCs threatened'...</td>\n      <td>['(CNN)Tensions remain high in Jewish communit...</td>\n      <td>Vandals damage 100 headstones at Jewish cemete...</td>\n      <td>Tue Feb 21 10:56:29 +0000 2017</td>\n      <td>us, Vandals damage 100 headstones at Jewish ce...</td>\n      <td>Tensions remain high in Jewish communities acr...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n      <td>0.200000</td>\n      <td>no-clickbait</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>['(cnn)tensions remain high in jewish communit...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4979</td>\n      <td>[]</td>\n      <td>Turkey dismisses 4,400 public servants in late...</td>\n      <td>829279322729021440</td>\n      <td>['Turkish President Erdogan reviews a guard of...</td>\n      <td>['Dismissals come hours after first phone call...</td>\n      <td>Turkey dismisses 4,400 public servants in late...</td>\n      <td>Wed Feb 08 10:42:52 +0000 2017</td>\n      <td>Turkey,Recep Tayyip Erdoğan,UK news,Donald Tru...</td>\n      <td>Dismissals come hours after first phone call b...</td>\n      <td>[0.0, 0.0, 0.33333333330000003, 0.0, 0.0]</td>\n      <td>0.066667</td>\n      <td>no-clickbait</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>['dismissals come hours after first phone call...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1448</td>\n      <td>['media/photo_818125834103857152.jpg']</td>\n      <td>21 \"Law And Order\" Tumblr posts that are fucki...</td>\n      <td>818185517921632256</td>\n      <td>[]</td>\n      <td>['If SVU isn’t your favorite, are you even a r...</td>\n      <td>21 \"Law And Order\" Tumblr Posts That Are Fucki...</td>\n      <td>Sun Jan 08 20:00:03 +0000 2017</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[0.6666666666000001, 0.33333333330000003, 1.0,...</td>\n      <td>0.600000</td>\n      <td>clickbait</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>['if svu isn’t your favorite, are you even a r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>381</td>\n      <td>[]</td>\n      <td>How one woman is using Tinder to take advantag...</td>\n      <td>847327280061919232</td>\n      <td>['istock-476739206.jpg']</td>\n      <td>['One user has received money from more than 2...</td>\n      <td>How women are using their Tinder matches to sc...</td>\n      <td>Thu Mar 30 05:59:00 +0000 2017</td>\n      <td>Tinder, dating apps, Dating, scam, Online dati...</td>\n      <td>How to create the perfect Tinder bio is one of...</td>\n      <td>[0.33333333330000003, 0.0, 1.0, 1.0, 0.6666666...</td>\n      <td>0.600000</td>\n      <td>clickbait</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>['one user has received money from more than 2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7226</td>\n      <td>[]</td>\n      <td>How the dinosaurs REALLY died out</td>\n      <td>820869672488890368</td>\n      <td>[\"Researchers say the 'big chill' had far more...</td>\n      <td>['The apocalyptic time that many believe wiped...</td>\n      <td>Is this how the dinosaurs REALLY died out? Res...</td>\n      <td>Mon Jan 16 05:45:55 +0000 2017</td>\n      <td>A,dark,frozen,Earth,Researchers,reconstruct,ap...</td>\n      <td>Researchers say the 'big chill' had far more c...</td>\n      <td>[0.33333333330000003, 1.0, 0.0, 1.0, 1.0]</td>\n      <td>0.666667</td>\n      <td>clickbait</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>['the apocalyptic time that many believe wiped...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9756</th>\n      <td>1296</td>\n      <td>['media/photo_822251094533292032.jpg']</td>\n      <td>This is what 100 years of women’s protest look...</td>\n      <td>822420412340387840</td>\n      <td>[]</td>\n      <td>['“Life’s a bitch. You’ve got to go out and ki...</td>\n      <td>This Is What 100 Years Of Women’s Protest Look...</td>\n      <td>Fri Jan 20 12:28:00 +0000 2017</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[1.0, 0.33333333330000003, 0.33333333330000003...</td>\n      <td>0.466667</td>\n      <td>no-clickbait</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>['“life’s a bitch. you’ve got to go out and ki...</td>\n    </tr>\n    <tr>\n      <th>9757</th>\n      <td>575</td>\n      <td>['media/photo_842072978926010368.jpg']</td>\n      <td>One #WBC2017 fan’s attempt to catch a home run...</td>\n      <td>842074484026798080</td>\n      <td>[]</td>\n      <td>['When\\xa0Yurendell de Caster of the Netherlan...</td>\n      <td>Netherlands Home Run Hits World Baseball Class...</td>\n      <td>Wed Mar 15 18:06:16 +0000 2017</td>\n      <td>MLB, Breaking News</td>\n      <td>When Yurendell de Caster of the Netherlands la...</td>\n      <td>[0.33333333330000003, 0.6666666666000001, 0.33...</td>\n      <td>0.466667</td>\n      <td>no-clickbait</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>['when yurendell de caster of the netherlands ...</td>\n    </tr>\n    <tr>\n      <th>9758</th>\n      <td>840</td>\n      <td>[]</td>\n      <td>Hunt master investigated by police after alleg...</td>\n      <td>834432577184292864</td>\n      <td>['Hunt master investigated after allegedly whi...</td>\n      <td>['A hunt master has been reported to police af...</td>\n      <td>Hunt master investigated by police after alleg...</td>\n      <td>Wed Feb 22 16:00:04 +0000 2017</td>\n      <td>News,Surrey,Surrey Police,Hunting,Standard,UK ...</td>\n      <td>A hunt master has been reported to police afte...</td>\n      <td>[0.33333333330000003, 0.0, 0.6666666666000001,...</td>\n      <td>0.400000</td>\n      <td>no-clickbait</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>['a hunt master has been reported to police af...</td>\n    </tr>\n    <tr>\n      <th>9759</th>\n      <td>3527</td>\n      <td>[]</td>\n      <td>\"In 2016’s politics and pop culture, women cam...</td>\n      <td>815386319782342656</td>\n      <td>['Dolores (Evan Rachel Wood) embraces her iden...</td>\n      <td>['This year, rather than making best-of lists,...</td>\n      <td>In 2016’s politics and pop culture, women came...</td>\n      <td>Sun Jan 01 02:37:02 +0000 2017</td>\n      <td>Hillary Clinton, Donald Trump, Game of Thrones...</td>\n      <td>Hillary Clinton, Cersei Lannister, Dolores Abe...</td>\n      <td>[1.0, 0.0, 1.0, 1.0, 0.33333333330000003]</td>\n      <td>0.666667</td>\n      <td>clickbait</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>['this year, rather than making best-of lists,...</td>\n    </tr>\n    <tr>\n      <th>9760</th>\n      <td>3305</td>\n      <td>[]</td>\n      <td>The GOP’s dilemma: As Obamacare repeal nears, ...</td>\n      <td>821458375280119808</td>\n      <td>['A woman holds an Obamacare sign in front of ...</td>\n      <td>[\"We are closer than we have ever been to the ...</td>\n      <td>The GOP’s dilemma: As Obamacare repeal nears, ...</td>\n      <td>Tue Jan 17 20:45:13 +0000 2017</td>\n      <td>the fix, obamacare, affordable care act, obama...</td>\n      <td>Trump promised “insurance for everybody.” And ...</td>\n      <td>[0.33333333330000003, 1.0, 0.0, 0.0, 0.0]</td>\n      <td>0.266667</td>\n      <td>no-clickbait</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>[\"we are closer than we have ever been to the ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>9761 rows × 16 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df is your DataFrame with 'truthClass' column\ndf_clickbait = df[df['truthClass'] == 'clickbait']\ndf_no_clickbait = df[df['truthClass'] == 'no-clickbait'].head(5000)\n\n# Concatenate the balanced DataFrames\nbalanced_df = pd.concat([df_clickbait, df_no_clickbait])\n\n# Shuffle the DataFrame to mix the classes\nbalanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Check the value counts for each class\nprint(balanced_df['truthClass'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-01-27T08:56:31.729421Z","iopub.execute_input":"2024-01-27T08:56:31.730308Z","iopub.status.idle":"2024-01-27T08:56:31.780879Z","shell.execute_reply.started":"2024-01-27T08:56:31.730263Z","shell.execute_reply":"2024-01-27T08:56:31.779643Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"truthClass\nno-clickbait    5000\nclickbait       4761\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"df=balanced_df","metadata":{"execution":{"iopub.status.busy":"2024-01-27T08:56:36.438755Z","iopub.execute_input":"2024-01-27T08:56:36.439157Z","iopub.status.idle":"2024-01-27T08:56:36.445611Z","shell.execute_reply.started":"2024-01-27T08:56:36.439125Z","shell.execute_reply":"2024-01-27T08:56:36.444452Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import re\n\nimport pandas as pd\nfrom concurrent.futures import ProcessPoolExecutor","metadata":{"execution":{"iopub.status.busy":"2024-01-27T08:56:38.707793Z","iopub.execute_input":"2024-01-27T08:56:38.708211Z","iopub.status.idle":"2024-01-27T08:56:38.712741Z","shell.execute_reply.started":"2024-01-27T08:56:38.708178Z","shell.execute_reply":"2024-01-27T08:56:38.711708Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    cleaned_text = re.sub(r'\\\\n|\\\\xa0|\\\\', ' ', text)\n    cleaned_text=cleaned_text.lower()\n    return cleaned_text\n\n# Load spaCy English language model\n\n\n# Load spaCy stop words\n\n\n# Batch processing using parallelism\nbatch_size = 100  # Adjust based on your data size\nnum_batches = len(df) // batch_size + 1\n\nwith ProcessPoolExecutor() as executor:\n    result = list(executor.map(preprocess_text, df['targetParagraphs']))\n\n# Add the cleaned string to the DataFrame\ndf['cleaned_string'] = result\n\n# Print the cleaned DataFrame\nprint(df[['targetParagraphs', 'cleaned_string']])","metadata":{"execution":{"iopub.status.busy":"2024-01-27T08:56:42.695847Z","iopub.execute_input":"2024-01-27T08:56:42.696244Z","iopub.status.idle":"2024-01-27T08:56:46.086212Z","shell.execute_reply.started":"2024-01-27T08:56:42.696213Z","shell.execute_reply":"2024-01-27T08:56:46.084997Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"                                       targetParagraphs  \\\n0     ['(CNN)Tensions remain high in Jewish communit...   \n1     ['Dismissals come hours after first phone call...   \n2     ['If SVU isn’t your favorite, are you even a r...   \n3     ['One user has received money from more than 2...   \n4     ['The apocalyptic time that many believe wiped...   \n...                                                 ...   \n9756  ['“Life’s a bitch. You’ve got to go out and ki...   \n9757  ['When\\xa0Yurendell de Caster of the Netherlan...   \n9758  ['A hunt master has been reported to police af...   \n9759  ['This year, rather than making best-of lists,...   \n9760  [\"We are closer than we have ever been to the ...   \n\n                                         cleaned_string  \n0     ['(cnn)tensions remain high in jewish communit...  \n1     ['dismissals come hours after first phone call...  \n2     ['if svu isn’t your favorite, are you even a r...  \n3     ['one user has received money from more than 2...  \n4     ['the apocalyptic time that many believe wiped...  \n...                                                 ...  \n9756  ['“life’s a bitch. you’ve got to go out and ki...  \n9757  ['when yurendell de caster of the netherlands ...  \n9758  ['a hunt master has been reported to police af...  \n9759  ['this year, rather than making best-of lists,...  \n9760  [\"we are closer than we have ever been to the ...  \n\n[9761 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-01-27T08:56:52.141460Z","iopub.execute_input":"2024-01-27T08:56:52.142131Z","iopub.status.idle":"2024-01-27T08:56:52.146354Z","shell.execute_reply.started":"2024-01-27T08:56:52.142099Z","shell.execute_reply":"2024-01-27T08:56:52.145463Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport pandas as pd\nimport torch\n\n# Load the model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\nmodel = model.to('cuda')  # if GPU is available\n# Load validation data\n\nval_texts = df['cleaned_string'].tolist()\nval_labels = df['truthClass'].map({'no-clickbait': 0, 'clickbait': 1}).tolist()  # convert sentiment to numeric\nval_texts=val_texts[1000:6000]\nval_labels=val_labels[1000:6000]\n\n# Initialize tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize data\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset for validation\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\nval_dataset = ReviewDataset(val_encodings, val_labels)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Predict with the model\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T09:30:12.012379Z","iopub.execute_input":"2024-01-27T09:30:12.012794Z","iopub.status.idle":"2024-01-27T09:32:11.228944Z","shell.execute_reply.started":"2024-01-27T09:30:12.012761Z","shell.execute_reply":"2024-01-27T09:32:11.228103Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification#, AdamW\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom torch.optim import AdamW\n\n\nimport time\n\n# Record start time\nstart_time = time.time()\n\n# Load data\n  # assuming sentiment is encoded as 0 (negative) and 1 (positive)\nreviews=val_texts\nlabels=val_labels\n# Split data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(reviews, labels, test_size=0.2)\n\n# Initialize tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize data\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create dataloaders\ntrain_dataset = ReviewDataset(train_encodings, train_labels)\nval_dataset = ReviewDataset(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\nmodel = model.to('cuda')  # if GPU is available\n\n# Initialize optimizer\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\n# Training loop\nimport time\nfrom tqdm import tqdm\nimport torch\n\n# Assuming you have a model, optimizer, and train_loader defined\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model to device\nmodel.to(device)\n\n# Number of epochs\nnum_epochs = 3\n\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\n# Record start time\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    model.train()\n    \n    # Create a tqdm progress bar for the training loader\n    tqdm_train_loader = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', dynamic_ncols=True)\n    \n    total_loss = 0.0\n    num_batches = len(train_loader)\n    \n    for batch in tqdm_train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Update tqdm progress bar\n        tqdm_train_loader.set_postfix({'Loss': loss.item()})\n    \n    # Calculate and display average loss after each epoch\n    average_loss = total_loss / num_batches\n    print(f'Epoch {epoch + 1}/{num_epochs} - Average Loss: {average_loss:.4f}')\n\n# Save the model\nmodel.save_pretrained('sentiment_model_DistilBERT')\n\n# Record end time\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T09:32:56.398482Z","iopub.execute_input":"2024-01-27T09:32:56.399288Z","iopub.status.idle":"2024-01-27T09:40:24.964171Z","shell.execute_reply.started":"2024-01-27T09:32:56.399251Z","shell.execute_reply":"2024-01-27T09:40:24.963079Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/3: 100%|██████████| 250/250 [01:47<00:00,  2.32it/s, Loss=0.641]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Average Loss: 0.6182\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.46] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 - Average Loss: 0.5258\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.35] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 - Average Loss: 0.4169\nTime required to fine-tune:  328.65401220321655\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm ","metadata":{"execution":{"iopub.status.busy":"2024-01-27T08:56:57.695464Z","iopub.execute_input":"2024-01-27T08:56:57.696417Z","iopub.status.idle":"2024-01-27T08:56:57.700429Z","shell.execute_reply.started":"2024-01-27T08:56:57.696381Z","shell.execute_reply":"2024-01-27T08:56:57.699491Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for epoch in range(3):  # number of epochs\n    model.train()\n    running_loss = 0.0\n    \n    # Wrap the train_loader with tqdm for a progress bar\n    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{3}', leave=False):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        # Display live-updating loss\n        \n    \n    # Print average loss after each epoch\n    avg_loss = running_loss / len(train_loader)\n    print(f'\\nEpoch {epoch + 1}/{3}, Avg Loss: {avg_loss:.4f}')\n\n# Save the model\nmodel.save_pretrained('sentiment_model_RoBERTa')\n\n# Record end time\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T19:18:32.611827Z","iopub.execute_input":"2024-01-25T19:18:32.612579Z","iopub.status.idle":"2024-01-25T19:38:59.088386Z","shell.execute_reply.started":"2024-01-25T19:18:32.612542Z","shell.execute_reply":"2024-01-25T19:38:59.087342Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/3, Avg Loss: 0.1516\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/3, Avg Loss: 0.1163\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/3, Avg Loss: 0.0950\nTime required to fine-tune:  3734.223074913025\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification#, AdamW\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom torch.optim import AdamW\n\nimport time\n# Record start time\nstart_time = time.time()\n\n\n# Load data\n # assuming sentiment is encoded as 0 (negative) and 1 (positive)\n\n# Split data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(val_texts, val_labels, test_size=0.2)\n\n# Initialize tokenizer\ntokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n\n# Tokenize data\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create dataloaders\ntrain_dataset = ReviewDataset(train_encodings, train_labels)\nval_dataset = ReviewDataset(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize model\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\nmodel = model.to('cuda')  # if GPU is available\n\n# Initialize optimizer\noptimizer = AdamW(model.parameters(), lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:36:44.842131Z","iopub.execute_input":"2024-01-25T18:36:44.842470Z","iopub.status.idle":"2024-01-25T18:36:57.875319Z","shell.execute_reply.started":"2024-01-25T18:36:44.842446Z","shell.execute_reply":"2024-01-25T18:36:57.874338Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\npredictions = []\ntrue_labels = []\nfor batch in val_loader:\n    input_ids = batch['input_ids'].to('cuda')\n    attention_mask = batch['attention_mask'].to('cuda')\n    labels = batch['labels'].to('cuda')\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n    predictions.extend(predicted_labels)\n    true_labels.extend(labels.cpu().numpy())\n\n# Calculate metrics\naccuracy = accuracy_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\nconf_matrix = confusion_matrix(true_labels, predictions)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'F1-score: {f1}')\nprint(f'Confusion matrix:\\n {conf_matrix}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Tokenize data\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset for validation\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\nval_dataset = ReviewDataset(val_encodings, val_labels)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Evaluate the model\nmodel.eval()\npredictions = []\ntrue_labels = []\nfor batch in val_loader:\n    input_ids = batch['input_ids'].to('cuda')\n    attention_mask = batch['attention_mask'].to('cuda')\n    labels = batch['labels'].to('cuda')\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n    predictions.extend(predicted_labels)\n    true_labels.extend(labels.cpu().numpy())\n\n# Calculate metrics\naccuracy = accuracy_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\nconf_matrix = confusion_matrix(true_labels, predictions)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'F1-score: {f1}')\nprint(f'Confusion matrix:\\n {conf_matrix}')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T08:59:03.665081Z","iopub.execute_input":"2024-01-27T08:59:03.665456Z","iopub.status.idle":"2024-01-27T08:59:04.285332Z","shell.execute_reply.started":"2024-01-27T08:59:03.665425Z","shell.execute_reply":"2024-01-27T08:59:04.284013Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mDistilBertTokenizerFast\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Tokenize data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m val_encodings \u001b[38;5;241m=\u001b[39m tokenizer(val_texts, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'DistilBertTokenizerFast' is not defined"],"ename":"NameError","evalue":"name 'DistilBertTokenizerFast' is not defined","output_type":"error"}]},{"cell_type":"code","source":"tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n\n# Tokenize data\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n# Create torch dataset for validation\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\nval_dataset = ReviewDataset(val_encodings, val_labels)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Evaluate the model\nmodel.eval()\npredictions = []\ntrue_labels = []\nfor batch in val_loader:\n    input_ids = batch['input_ids'].to('cuda')\n    attention_mask = batch['attention_mask'].to('cuda')\n    labels = batch['labels'].to('cuda')\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n    predictions.extend(predicted_labels)\n    true_labels.extend(labels.cpu().numpy())\n\n# Calculate metrics\naccuracy = accuracy_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\nconf_matrix = confusion_matrix(true_labels, predictions)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'F1-score: {f1}')\nprint(f'Confusion matrix:\\n {conf_matrix}')","metadata":{"execution":{"iopub.status.busy":"2024-01-25T19:41:10.734970Z","iopub.execute_input":"2024-01-25T19:41:10.735353Z","iopub.status.idle":"2024-01-25T19:41:47.297134Z","shell.execute_reply.started":"2024-01-25T19:41:10.735324Z","shell.execute_reply":"2024-01-25T19:41:47.296097Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Accuracy: 0.7142857142857143\nF1-score: 0.6934065934065934\nConfusion matrix:\n [[764 235]\n [323 631]]\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(20):\n    model.train()\n    \n    # Create a tqdm progress bar for the training loader\n    tqdm_train_loader = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', dynamic_ncols=True)\n    \n    total_loss = 0.0\n    num_batches = len(train_loader)\n    \n    for batch in tqdm_train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Update tqdm progress bar\n        tqdm_train_loader.set_postfix({'Loss': loss.item()})\n    \n    # Calculate and display average loss after each epoch\n    average_loss = total_loss / num_batches\n    print(f'Epoch {epoch + 1}/{num_epochs} - Average Loss: {average_loss:.4f}')\n\n# Save the model\nmodel.save_pretrained('sentiment_model_DistilBERT')\n\n# Record end time\nend_time = time.time()\n\nprint(\"Time required to fine-tune: \", end_time - start_time)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T09:40:24.965825Z","iopub.execute_input":"2024-01-27T09:40:24.966140Z","iopub.status.idle":"2024-01-27T09:56:52.146684Z","shell.execute_reply.started":"2024-01-27T09:40:24.966114Z","shell.execute_reply":"2024-01-27T09:56:52.145440Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Epoch 1/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.172] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Average Loss: 0.2622\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.414]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 - Average Loss: 0.1232\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.408]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 - Average Loss: 0.0732\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.0191] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/3 - Average Loss: 0.0541\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.14]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/3 - Average Loss: 0.0392\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.00832]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/3 - Average Loss: 0.0374\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.209]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/3 - Average Loss: 0.0360\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/3: 100%|██████████| 250/250 [01:50<00:00,  2.27it/s, Loss=0.00807] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/3 - Average Loss: 0.0369\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/3:  95%|█████████▌| 238/250 [01:45<00:05,  2.26it/s, Loss=0.0232]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2024-01-27T09:58:50.147648Z","iopub.execute_input":"2024-01-27T09:58:50.148083Z","iopub.status.idle":"2024-01-27T09:58:50.152485Z","shell.execute_reply.started":"2024-01-27T09:58:50.148042Z","shell.execute_reply":"2024-01-27T09:58:50.151578Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"Pkl_Filename = \"Pickle_RL_Model.pkl\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(model, file)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T09:59:04.477801Z","iopub.execute_input":"2024-01-27T09:59:04.478214Z","iopub.status.idle":"2024-01-27T09:59:05.093448Z","shell.execute_reply.started":"2024-01-27T09:59:04.478184Z","shell.execute_reply":"2024-01-27T09:59:05.092335Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}